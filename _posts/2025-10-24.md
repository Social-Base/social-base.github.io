---
layout: default
title: Surveys
date:
---
## Overview

Surveys are systematic data collection instruments that use standardized questions to gather information from a sample of respondents about their behaviors, attitudes, beliefs, experiences, or characteristics. Unlike interviews where questions may evolve organically, surveys maintain consistency across all respondents, enabling researchers to quantify patterns, test hypotheses, and make statistical inferences about larger populations. This standardization is both the method's greatest strength and a key point of methodological debate.

**Type:** Primarily Quantitative (though can include qualitative open-ended questions)

**Typical Duration:** 2-12 months (including design, piloting, data collection, and analysis)

---

## Key Characteristics

- **Standardization**: All respondents encounter identical or systematically varied questions, reducing interviewer effects but potentially missing contextual nuances
- **Scalability**: Can efficiently reach hundreds to millions of respondents, though response rates and sample representativeness remain persistent challenges
- **Structured response formats**: Predominantly uses closed-ended questions (multiple choice, Likert scales, ranking) that facilitate quantitative analysis but constrain how respondents can express themselves
- **Statistical generalizability**: When properly sampled, results can be projected to broader populations with quantifiable margins of error—a claim qualitative methods typically cannot make

---

## When to Use This Method

**Best suited for:**

- Estimating prevalence, frequency, or distribution of phenomena across populations
- Testing relationships between variables with sufficient statistical power
- Comparing subgroups or tracking changes over time with standardized metrics
- Research questions requiring breadth over depth—understanding patterns across many cases rather than deep exploration of individual cases

**Research questions it addresses:**

- "What percentage of college students experience food insecurity, and how does this vary by socioeconomic status?"
- "Is there a relationship between social media usage and reported life satisfaction among adolescents?"
- "How have public attitudes toward climate policy shifted over the past decade?"
- "Which factors best predict voter turnout in local elections?"

**When surveys may be inadequate:** Exploring processes, meanings, or complex experiences that respondents cannot easily articulate in predetermined categories; studying populations where literacy or question comprehension is problematic; investigating sensitive topics where social desirability bias severely distorts responses.

---

## Disciplines & Fields

**Commonly used in:**

- Sociology and demography (population studies, inequality research, social mobility)
- Political science (public opinion, voting behavior, political attitudes)
- Psychology (personality assessment, attitude measurement, behavior frequency)
- Public health and epidemiology (health behaviors, risk factors, disease prevalence)
- Economics (consumer behavior, labor market studies, willingness-to-pay)
- Education research (student experiences, program evaluation, learning outcomes)
- Market research and organizational studies (customer satisfaction, employee engagement)

---

## Step-by-Step Process

1. **Conceptualization & Question Development**
    - Define research objectives and identify key constructs to measure
    - Review existing validated instruments versus creating new questions
    - Decide on question types, response formats, and scale structures
    - Critical consideration: Are you measuring the construct you intend to measure, or a proxy? The gap between theoretical concepts and operational measures is rarely perfect.
2. **Instrument Design & Pilot Testing**
    - Organize questions logically, considering question order effects and survey flow
    - Develop clear instructions and ensure accessible language appropriate to your population
    - Conduct cognitive interviews to identify comprehension problems, ambiguous wording, or missing response options
    - Pilot test with 20-50 respondents from your target population to assess completion time, dropout rates, and response variability
    - Methodological debate: How much piloting is sufficient? Resource constraints often limit this crucial phase.
3. **Sampling & Recruitment**
    - Define target population and select sampling strategy (probability vs. non-probability)
    - Determine required sample size based on desired precision, expected effect sizes, and subgroup analyses
    - Implement recruitment procedures and track response rates across demographic groups
    - Crucial question: Who is missing from your sample, and how might their absence bias your results?
4. **Data Collection**
    - Deploy survey via chosen mode (online, mail, phone, in-person) or mixed-mode approach
    - Monitor completion rates, identify patterns in item non-response, and implement follow-up strategies
    - Maintain data quality protocols to identify fraudulent responses, speeders, or straight-lining
    - Ethical consideration: Balance persistence in follow-up with respect for non-respondents' autonomy
5. **Data Cleaning & Preparation**
    - Code open-ended responses, reverse-score negatively worded items, compute composite scales
    - Address missing data through listwise deletion, imputation, or specialized missing data methods
    - Check for multivariate outliers, illogical response patterns, and data entry errors
    - Apply sampling weights if using complex survey designs to correct for unequal selection probabilities
6. **Analysis & Interpretation**
    - Conduct descriptive statistics, bivariate analyses, and multivariate modeling as appropriate
    - Test reliability of scales (Cronbach's alpha, test-retest reliability) and validity evidence
    - Interpret findings within the context of measurement error, sampling variability, and potential biases
    - Philosophical tension: Statistical significance versus substantive significance—when do findings actually matter?

---

## Data Collection

**What you'll collect:**

- Numeric responses to closed-ended questions (categorical, ordinal, interval, ratio data)
- Text responses to open-ended questions (requiring qualitative coding)
- Paradata (response times, device type, breakoff points, IP addresses) that can reveal data quality issues
- Auxiliary information (sampling frame variables, geographic indicators) for weighting and adjustment

**Tools & materials needed:**

- Survey platform (Qualtrics, SurveyMonkey, LimeSurvey, REDCap) or traditional paper instruments
- Recruitment materials (invitation emails, consent forms, incentive tracking systems)
- Data management software (SPSS, Stata, R, Python) for analysis
- Sampling frame or access to population (email lists, postal addresses, phone numbers, panel services)
- Budget for incentives, which can significantly improve response rates but raise questions about coercion and sample composition

---

## Sample Size Considerations

**Typical sample size:** Highly variable—from 200 for basic descriptive studies to 10,000+ for complex subgroup analyses or rare outcome studies

**Factors affecting sample size:**

- **Desired precision**: Smaller margins of error require larger samples (relationship is not linear; quadrupling sample size halves margin of error)
- **Population heterogeneity**: More diverse populations require larger samples to capture variability
- **Subgroup analyses**: Each subgroup comparison requires adequate statistical power
- **Expected response rates**: Initial sample must account for non-response (e.g., recruit 2,000 if expecting 30% response rate to achieve 600 completions)
- **Analytical complexity**: Multivariate models, structural equation modeling, and mediation analyses require larger samples than simple descriptive statistics
- **Rare phenomena**: Studying low-prevalence conditions requires larger samples or targeted sampling strategies

**The power analysis debate:** Many published surveys report post-hoc power analyses or inadequate sample size justifications. Pre-registered power analyses are increasingly expected but remain uncommon in practice.

---

## Analysis Approaches

**Common analytical techniques:**

- **Descriptive statistics**: Frequencies, percentages, means, medians—the foundation for understanding your data distribution and identifying data anomalies
- **Bivariate analysis**: Cross-tabulations with chi-square tests, correlation coefficients, t-tests, ANOVA—exploring relationships between two variables while acknowledging that correlation doesn't imply causation
- **Regression modeling**: Linear, logistic, ordinal, multilevel models that control for confounding variables and estimate adjusted associations—but model specification choices critically shape conclusions
- **Factor analysis and structural equation modeling**: Data reduction techniques and latent variable models that test theoretical structures, though these methods rest on strong distributional assumptions
- **Weighting and stratification**: Adjusting for complex sampling designs or correcting for non-response bias—necessary but imperfect solutions to sampling imperfections
- **Comparative analysis**: Subgroup comparisons, trend analysis over time, cross-national comparisons that reveal patterns but require careful attention to measurement equivalence

**Software tools:**

- SPSS (user-friendly interface, widely used in social sciences)
- Stata (powerful for survey statistics, preferred in economics and epidemiology)
- R (free, flexible, reproducible, but steeper learning curve—packages like survey, lavaan, tidyverse)
- Python (increasingly popular, particularly pandas, statsmodels, scikit-learn)
- SAS (common in government and pharmaceutical research)
- Mplus (specialized for structural equation modeling and latent variable analysis)

---

## Strengths

✓ **Efficiency at scale**: Can gather data from thousands of respondents far more quickly and cheaply than qualitative methods, making population-level inferences feasible
✓ **Standardization enables comparison**: Identical questions across respondents, time points, or geographic regions allow for systematic comparisons that qualitative research struggles to achieve
✓ **Statistical rigor and generalizability**: Probability sampling permits inference to broader populations with quantifiable uncertainty, addressing the "how many?" and "how much?" questions
✓ **Replicability**: Standardized instruments can be repeated by other researchers, facilitating cumulative knowledge building and meta-analyses
✓ **Perceived legitimacy**: Quantitative survey findings often carry particular weight with policymakers and stakeholders, though this privileging of numbers deserves critical reflection
✓ **Anonymity can increase honesty**: Respondents may disclose sensitive information more readily in surveys than in face-to-face interviews, though this depends heavily on trust and questionnaire design

---

## Limitations

✗ **Reductionism**: Complex social phenomena must be simplified into measurable indicators, potentially losing contextual richness, nuance, and meaning—the map is not the territory
✗ **Response rate crisis**: Survey response rates have declined dramatically (often 5-20% for general population surveys), raising serious questions about non-response bias and generalizability
✗ **Measurement error is inevitable**: Questions may be misunderstood, response options may not fit respondents' experiences, and recall of past behaviors is often inaccurate
✗ **Social desirability and satisficing**: Respondents may answer in socially acceptable ways rather than truthfully, or expend minimal cognitive effort, particularly in long surveys
✗ **Causality remains elusive**: Most surveys are cross-sectional, capturing associations at one time point but unable to definitively establish causal direction (does anxiety cause social media use, or vice versa?)
✗ **Respondent burden**: Survey fatigue is real; asking too much can degrade data quality and harm future research participation rates
✗ **Coverage bias**: Not everyone has equal probability of selection (e.g., online surveys exclude those without internet access), and these exclusions are often systematic

---

## Ethical Considerations

- **Informed consent complexities**: How much information about the study's purpose should be revealed? Too much may bias responses; too little may be ethically problematic. The tension between methodological rigor and full transparency is unavoidable.
- **Voluntary participation versus incentive effects**: Paying respondents can improve response rates but may constitute undue influence, particularly for economically vulnerable populations. What amount is "too much"?
- **Privacy and confidentiality**: Collecting identifying information versus anonymous responses involves tradeoffs between follow-up capabilities, data linkage possibilities, and respondent protection. De-identification is harder than it appears.
- **Question sensitivity**: Asking about traumatic experiences, illegal behaviors, or stigmatized conditions can cause distress. Researchers must balance scientific value against potential psychological harm.
- **Results dissemination**: Who has access to findings? Aggregate results may mask important subgroup vulnerabilities, while detailed breakdowns may enable identification in small communities.
- **Digital divides**: Online surveys systematically exclude populations without internet access or digital literacy, potentially amplifying existing inequalities in whose voices are heard in research.

---

## Validity & Reliability

**Ensuring quality:**

**Reliability (consistency of measurement):**

- Internal consistency reliability (Cronbach's alpha, omega coefficients) for multi-item scales—though high reliability doesn't guarantee validity
- Test-retest reliability by administering the survey twice to the same respondents—but beware memory effects and genuine attitude change
- Inter-rater reliability for coding open-ended responses—subjective coding requires transparency and calibration

**Validity (measuring what you intend to measure):**

- **Content validity**: Do questions comprehensively cover the construct? Expert review and cognitive interviewing help assess this.
- **Construct validity**: Does the measure relate to other variables as theory predicts? Confirmatory factor analysis and known-groups validation provide evidence.
- **Criterion validity**: Does the measure correlate with established gold standards or predict relevant outcomes?
- **Face validity**: Do questions appear relevant to respondents? Low face validity may increase non-response or frivolous answers.

**Ongoing debates**:

- Validity is not a property of an instrument but of the interpretations and uses of scores in specific contexts. A valid measure for one population may not be valid for another.
- Psychometric purists emphasize extensive validation before use, while pragmatists note that this can delay timely research on emerging phenomena. Where's the balance?

---

## Real-World Example

**Study title:** "Social Capital and Mental Health: An Updated Interdisciplinary Review of Primary Evidence"

**Researcher(s):** Ehsan Ehsan & Joseph Reeves De-Graft Acquah-Johnson (drawing on General Social Survey data)

**What they did:** The researchers utilized longitudinal survey data from the U.S. General Social Survey, which has tracked social attitudes and behaviors since 1972. They examined relationships between various forms of social capital (community participation, trust, social networks) and mental health outcomes across multiple waves of data collection. The survey's standardized questions allowed them to compare trends over decades and across demographic groups, while controlling for potential confounders like socioeconomic status and health conditions. The large sample size (thousands of respondents per wave) enabled sophisticated statistical modeling to disentangle complex relationships.

**Key findings:** Different dimensions of social capital showed varying associations with mental health. Structural social capital (formal group participation) showed weaker associations than cognitive social capital (trust and reciprocity norms). The relationships varied by age, gender, and socioeconomic context, highlighting that broad generalizations about "social capital" may obscure important heterogeneity.

**Citation:** Ehsan, A., & De-Graft Acquah-Johnson, J. R. (2024). Social Capital and Mental Health: An Updated Interdisciplinary Review of Primary Evidence. _International Journal of Mental Health_, 53(1), 98-129.

---

## Getting Started: Practical Tips

1. **Start with existing measures when possible**: Review databases like the Health and Psychosocial Instruments database or discipline-specific repositories. Using validated instruments saves time, enables comparisons across studies, and avoids reinventing the wheel—but don't use measures uncritically without assessing their fit for your population.
2. **Write clear, unambiguous questions**: Avoid double-barreled questions ("Do you support increased funding for education and healthcare?"—which are two questions), loaded language, acronyms, and technical jargon. Every word matters; subtle changes can dramatically shift responses.
3. **Consider question order strategically**: Place sensitive questions later after rapport is established, be aware of priming effects where early questions influence later responses, and group related items logically while occasionally reversing scales to detect acquiescence bias.
4. **Pilot test extensively, not perfunctorily**: Conduct cognitive interviews where respondents think aloud while answering, don't just ask "did you understand?"—people often claim to understand when they don't. Test with diverse respondents who represent your target population's range.
5. **Design for your analysis**: Before finalizing questions, sketch out your intended analysis tables and models. If you won't be able to analyze the data in ways that answer your research questions, revise the instrument.
6. **Build in data quality checks**: Include attention checks (instructed response items like "Please select 'strongly agree' for this question"), add open-ended "other (please specify)" options to capture unexpected responses, and track completion times to identify speeders.
7. **Be realistic about respondent burden**: Survey length is the enemy of completion. Aim for 10-15 minutes maximum for general population surveys. Every additional minute dramatically increases dropout rates. Prioritize ruthlessly—what can you live without?
8. **Plan for non-response from the beginning**: Who will be hardest to reach? How will their absence affect your conclusions? Consider targeted oversampling, mixed-mode designs, or weighting strategies during the design phase, not as afterthoughts.

---

## Common Mistakes to Avoid

⚠️ **Asking respondents what they cannot know**: Questions about precise frequencies ("How many times did you exercise last year?") or future intentions ("Will you vote in elections 10 years from now?") often produce unreliable data. Recognize the limits of self-report.
⚠️ **Confusing statistical significance with importance**: A large sample can make trivial effects statistically significant. Always report and interpret effect sizes. A p-value tells you whether an effect exists, not whether it matters.
⚠️ **Treating ordinal data as interval**: Likert scales (strongly disagree to strongly agree) are ordered categories, not true numeric scales with equal intervals. While treating them as continuous is common, it rests on assumptions worth acknowledging. Debate continues about whether means of Likert items are meaningful.
⚠️ **Ignoring non-response bias**: High response rates don't guarantee representativeness, and low rates don't automatically invalidate findings—but you must investigate whether respondents differ systematically from non-respondents and address this in your interpretation.
⚠️ **Over-surveying the same populations**: College students, online panel members, and convenience samples are overused. Consider whether your findings will generalize beyond these easily accessible but potentially unrepresentative groups.
⚠️ **Failing to pre-register**: For confirmatory research, pre-registering hypotheses, analysis plans, and sample sizes increases transparency and guards against p-hacking, HARKing (Hypothesizing After Results are Known), and researcher degrees of freedom that can produce false positives.

---

## Resource Requirements

**Time:**

- Small project (single institution, 200 respondents): 3-6 months
- Medium project (regional study, 1,000 respondents): 6-12 months
- Large project (national probability sample, 5,000+ respondents): 1-2 years Add time for IRB approval (1-3 months), publications (6-18 months), and longer if developing new instruments

**Budget:** Highly variable

- Online convenience sample: $500-$5,000 (platform fees, incentives)
- Phone or mail survey of probability sample: $20,000-$100,000+
- National face-to-face survey: $500,000-$5,000,000+ Major costs: incentives ($5-$50 per respondent), survey platform, personnel time, sampling frame access, and data collection mode

**Skills needed:**

- Questionnaire design and measurement theory
- Sampling methodology and statistical inference
- Statistical software proficiency
- Understanding of validity, reliability, and psychometrics
- Critical thinking about measurement error and bias
- Clear writing for both questions and interpretation

**Team size:**

- Minimum: 1-2 (small student project)
- Typical academic study: 2-5 (PI, graduate students, research assistants)
- Large-scale survey: 10-100+ (project directors, methodologists, programmers, field interviewers, data managers)

---

## Combining with Other Methods

**Works well with:**

- **Qualitative interviews or focus groups first**: Use exploratory qualitative work to identify salient issues, develop culturally appropriate question wording, and discover response options you wouldn't have anticipated. Sequential explanatory designs then use interviews post-survey to interpret surprising findings or understand outlier cases.
- **Administrative data or behavioral traces**: Triangulate self-reported behaviors with objective records (sales data, medical records, social media activity) to assess reporting accuracy and identify discrepancies that may reveal social desirability bias or recall error.
- **Experiments embedded within surveys**: Survey experiments randomly vary question wording, framing, or vignette details to establish causal effects of messaging or information—combining external validity of survey samples with internal validity of experimental designs.
- **Ethnographic observation**: Surveys tell you what people say they do; ethnography shows what they actually do. The comparison reveals gaps between attitudes and behavior, espoused values and enacted practices.

---

## Further Reading

**Essential texts:**

1. Fowler, F. J. (2014). _Survey Research Methods_ (5th ed.). SAGE Publications. [Accessible introduction covering design, sampling, and question writing with practical examples]
2. Groves, R. M., et al. (2009). _Survey Methodology_ (2nd ed.). Wiley. [Comprehensive advanced textbook covering Total Survey Error framework, sampling theory, and modern methodological challenges]
3. Dillman, D. A., Smyth, J. D., & Christian, L. M. (2014). _Internet, Phone, Mail, and Mixed-Mode Surveys: The Tailored Design Method_ (4th ed.). Wiley. [Practical guide to maximizing response rates and data quality across survey modes]
4. Tourangeau, R., Rips, L. J., & Rasinski, K. (2000). _The Psychology of Survey Response_. Cambridge University Press. [Cognitive psychological approach to understanding how respondents interpret and answer questions]
5. Lyberg, L., et al. (Eds.). (2024). _Total Survey Error in Practice_. Wiley. [Contemporary treatment of error sources in surveys and strategies for reducing them]

**Online resources:**

- [American Association for Public Opinion Research (AAPOR)](https://aapor.org/): Standards, best practices, and transparency guidelines at aapor.org
- Pew Research Center Methods: Transparent documentation of major survey methodologies at pewresearch.org/methods
- Question Bank databases: [UK Data Service Question Bank](UK Data Service Question Bank), [NORC Question Bank for cross-study comparisons](https://gss.norc.org/get-documentation/questionnaires.html)

**Key journals:**

- _[Public Opinion Quarterly](https://academic.oup.com/poq)_ (methodological innovations and applications)
- _[Journal of Survey Statistics and Methodology](https://academic.oup.com/jssam)_ (statistical and methodological advances)
- _Survey Research Methods_ (open-access European journal)

**Video tutorials:**

- Coursera: "Questionnaire Design for Social Surveys" by University of Michigan
- YouTube: [AAPOR webinar series](https://www.youtube.com/@aaporHQvideo) on emerging methodological challenges