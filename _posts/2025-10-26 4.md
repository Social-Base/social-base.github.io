---
layout: post
title: Experiments
---


## Overview

Experiments are systematic investigations in which researchers deliberately manipulate one or more independent variables (treatments, interventions, conditions) while controlling other factors, then observe effects on dependent variables (outcomes of interest) to establish causal relationships. Distinguished from observational research by this active manipulation and control, experiments are considered the "gold standard" for causal inference—enabling researchers to determine not merely that variables are associated but that one actually causes changes in another. The logic is elegant: if groups differ only in the manipulated variable (through random assignment and control), any subsequent differences in outcomes can be attributed to that manipulation with high confidence, ruling out alternative explanations. Yet this experimental ideal faces practical, ethical, and epistemological challenges: Many important social phenomena cannot or should not be manipulated; laboratory control produces artificial settings potentially limiting generalizability; random assignment may be impossible or unethical; and the reductionist approach of isolating variables may miss the very complexity that matters in real-world contexts. Contemporary experimental social science includes diverse variants—laboratory experiments maximizing control, field experiments in natural settings, natural experiments exploiting naturally occurring variation, and online experiments leveraging digital platforms—each navigating different trade-offs between internal validity (causal certainty) and external validity (real-world applicability).

**Type:** Quantitative (primarily, though can include qualitative components in mixed methods designs)

**Typical Duration:** 3-18 months (design, implementation, data collection, analysis), though some experiments take minutes while longitudinal experiments can span years

---

## Key Characteristics

- **Manipulation of independent variable(s)**: The researcher actively intervenes, creating different conditions or treatments rather than passively observing naturally occurring variation—this control is what enables causal inference
- **Random assignment**: Participants are randomly allocated to experimental conditions, ensuring groups are probabilistically equivalent on all characteristics except the manipulation—controlling for confounds through randomization rather than measurement
- **Control group or condition**: Typically includes a baseline condition receiving no treatment, placebo, or standard treatment for comparison—isolating the effect of the manipulation
- **Control over extraneous variables**: Through random assignment, standardization of procedures, controlled settings, or statistical control, experiments minimize alternative explanations for observed effects
- **Quantitative measurement**: Outcomes are typically measured numerically enabling statistical testing of whether differences between conditions exceed chance variation
- **Theory testing**: Usually designed to test specific hypotheses derived from theory—deductive approach examining whether predicted relationships hold empirically

---

## When to Use This Method

**Best suited for:**

- Establishing causal relationships—determining whether X causes Y, not merely whether they correlate
- Testing specific theoretical predictions about how manipulations will affect outcomes
- Isolating effects of particular variables from confounding factors through experimental control
- Research questions about "what would happen if"—counterfactual comparisons between presence and absence of treatments
- Evaluating interventions, programs, or policies to determine whether they produce intended effects
- Situations where manipulation is feasible, ethical, and meaningful—researchers can create the conditions of interest
- Questions requiring precision about effect sizes—not just whether effects exist but how large they are

**Research questions it addresses:**

- "Does exposure to counter-stereotypical exemplars reduce implicit bias?"
- "Do financial incentives increase voter turnout, and if so, what amount is most effective?"
- "Does collaborative learning improve student outcomes compared to individual study?"
- "Will providing social norms information reduce energy consumption?"
- "Does framing health messages in terms of gains versus losses affect behavior change?"

**When experiments may be inadequate:** Studying phenomena that cannot ethically or practically be manipulated (trauma, discrimination, poverty); understanding complex social processes where reductionist isolation distorts meaning; research requiring deep contextual understanding rather than causal isolation; topics where artificial experimental settings produce results that don't generalize to natural contexts; historical questions or rare events that cannot be recreated; when you need to know not just whether something works but how and why it works in complex real-world settings. Experiments excel at internal validity but often sacrifice ecological validity, thick description, and attention to mechanisms.

---

## Disciplines & Fields

**Commonly used in:**

- Psychology (cognitive, social, developmental, clinical psychology—foundational method)
- Economics (experimental economics, behavioral economics, game theory)
- Political science (political behavior, voting, framing effects, campaign messaging)
- Education (educational interventions, pedagogical methods, learning technologies)
- Public health (intervention trials, health behavior change, prevention programs)
- Communication (media effects, persuasion, framing, agenda-setting)
- Marketing and consumer behavior (advertising effectiveness, product preferences, pricing)
- Organizational behavior (leadership, motivation, team dynamics, decision-making)
- Sociology (social norms, inequality, discrimination, collective behavior)
- Neuroscience (cognitive neuroscience, experimental paradigms combined with brain imaging)
- Human-computer interaction (interface design, usability, user experience)

---

## Step-by-Step Process

1. **Conceptualization and Hypothesis Development**
    
    - Review relevant literature and theory identifying research questions and testable hypotheses
    - Specify independent variable(s) to manipulate and dependent variable(s) to measure
    - Develop clear, falsifiable predictions about how manipulation will affect outcomes
    - Consider mechanisms: Why should this manipulation produce effects? What's the causal process?
    - Critical decision: What exactly will you manipulate, and does this manipulation validly represent the theoretical construct? For example, studying "stress" by having participants give speeches versus solve math problems creates very different operationalizations with potentially different effects.
2. **Operationalization of Variables**
    
    - **Independent variable**: How will you create different conditions? What specific manipulations, stimuli, or treatments?
    - **Dependent variable**: How will you measure outcomes? What instruments, scales, behavioral measures, or physiological indicators?
    - **Manipulation check**: How will you verify that participants perceived the manipulation as intended?
    - **Construct validity**: Do your operational definitions capture the theoretical constructs you claim to study?
    - Design considerations: More than two conditions often valuable—dose-response relationships, multiple comparison conditions, testing mechanisms
3. **Determine Experimental Design**
    
    - **Between-subjects design**: Different participants in each condition—simpler analysis but requires more participants and vulnerable to pre-existing differences (mitigated by randomization)
    - **Within-subjects (repeated measures) design**: Same participants experience all conditions—more statistical power, controls for individual differences, but vulnerable to order effects, carryover, practice, fatigue
    - **Mixed designs**: Combining between and within-subjects factors—more complex but can address multiple questions simultaneously
    - **Factorial designs**: Manipulating multiple independent variables simultaneously—reveals not just main effects but interactions (whether one variable's effect depends on another)
    - Consider: How many conditions? What sample size per condition? What order will participants experience conditions (if within-subjects)?
4. **Power Analysis and Sample Size Determination**
    
    - Conduct a priori power analysis determining required sample size to detect expected effect with adequate probability
    - Typical targets: 80% power to detect effect at p < .05 significance level
    - Considerations: Expected effect size (from literature or pilot), desired power, significance threshold, number of conditions, within vs. between-subjects
    - Larger samples needed for: smaller effects, more conditions, more statistical tests, exploratory analyses
    - Pre-registration increasingly expected: Document design and analysis plan before data collection, preventing questionable research practices
5. **Develop Materials and Procedures**
    
    - Create experimental materials: Stimuli, instructions, questionnaires, manipulations
    - Write detailed protocol: Exact procedures, timing, randomization procedures, what researchers say/do
    - Standardization: Ensure all participants in same condition receive identical experience—reduces error variance
    - Pilot testing: Run preliminary version with small sample, assess whether manipulations work, identify problems, refine procedures
    - Manipulation checks: Build in verification that manipulation was perceived as intended
    - Cover story (if deception used): Plausible rationale for study hiding true purpose—though increasingly problematic ethically
6. **Obtain Ethical Approval**
    
    - Submit protocol to Institutional Review Board (IRB) or ethics committee
    - Address: Informed consent, potential risks, deception justification (if used), debriefing, data protection, vulnerable populations
    - Random assignment raises ethical issues: Is it fair that some receive treatment others don't? Equipoise (genuine uncertainty about which condition is better) justifies randomization for many but not all contexts
    - Deception: Increasingly scrutinized—can it be justified? Is debriefing adequate? Does it undermine informed consent?
7. **Recruit and Screen Participants**
    
    - Determine eligibility criteria and recruitment sources
    - Common sources: Student samples (convenient but limited generalizability), online platforms (MTurk, Prolific—broader but quality concerns), community samples (more representative but expensive), clinical populations (for applied research)
    - Screen for eligibility and obtain informed consent
    - The WEIRD problem (Western, Educated, Industrialized, Rich, Democratic): Most psychological research uses samples unrepresentative of humanity—limits generalizability
8. **Random Assignment to Conditions**
    
    - Use true randomization (random number generator, not haphazard assignment)
    - Common methods: Simple random assignment, block randomization (ensuring equal numbers per condition), stratified randomization (balancing on key characteristics)
    - Verify randomization worked: Check whether conditions differ on demographics or pre-test measures—large differences suggest randomization failure
    - Maintain blindness when possible: Experimenters shouldn't know participants' conditions (preventing unconscious bias), though often impractical
9. **Administer the Experiment**
    
    - Follow protocol precisely, maintaining standardization
    - Randomly order conditions (in within-subjects designs) or counterbalance (systematic rotation) to control order effects
    - Collect manipulation check data verifying participants perceived manipulation as intended
    - Monitor for problems: Participants not following instructions, equipment failures, environmental disruptions
    - Maintain careful records: Who was in which condition, any deviations from protocol, contextual observations
10. **Debrief Participants**
    

- Explain study's true purpose, particularly if deception was used
- Address any concerns, discomfort, or questions
- Provide educational value when possible—participants should learn something
- Offer resources if study involved potentially distressing content
- For deception studies: Ethical debriefing must reveal deception, explain why necessary, ensure participants leave comfortable
- Some argue debriefing is insufficient to remedy deception's ethical problems—ongoing debate

11. **Data Analysis**

- **Check assumptions**: Ensure data meet statistical test assumptions (normality, homogeneity of variance, independence)
- **Manipulation checks**: Verify manipulation worked as intended—if not, results are uninterpretable
- **Primary analyses**: Test hypotheses using appropriate statistics
    - Two groups: Independent samples t-test (between-subjects) or paired t-test (within-subjects)
    - Multiple groups: ANOVA, ANCOVA (with covariates), MANOVA (multiple DVs)
    - Factorial designs: Factorial ANOVA testing main effects and interactions
    - Follow-up tests: Post-hoc comparisons, contrast tests, simple effects
- **Effect sizes**: Report standardized measures (Cohen's d, partial eta-squared, etc.)—not just statistical significance but practical magnitude
- **Exploratory analyses**: Additional analyses not pre-registered should be clearly labeled as exploratory to distinguish from confirmatory tests

12. **Interpretation and Contextualization**

- Do results support hypotheses? What about alternative explanations?
- Consider mechanisms: Why did the manipulation produce (or fail to produce) effects?
- Assess generalizability: To what populations, settings, and operationalizations do findings apply?
- Compare to prior literature: Replication, extension, or contradiction?
- Address limitations: What couldn't the experiment tell you? What contextual factors were ignored?
- Practical significance: Statistical significance doesn't equal importance—are effects large enough to matter?

---

## Data Collection

**What you'll collect:**

- **Demographic information**: Age, gender, education, relevant characteristics for checking randomization and examining moderators
- **Pre-test measures**: Baseline assessments before manipulation (in some designs)
- **Manipulation check data**: Verification that participants perceived manipulation as intended
- **Dependent variable measurements**: Primary outcomes of interest—can include:
    - Self-report scales (attitudes, perceptions, intentions, emotions)
    - Behavioral measures (choices, performance, reaction times, donations, votes)
    - Physiological measures (heart rate, skin conductance, eye-tracking, brain imaging)
    - Observational coding (social behaviors, facial expressions, verbal content)
- **Mediator and moderator variables**: Potential mechanisms or boundary conditions
- **Post-test measures**: Outcomes measured after manipulation
- **Debriefing responses**: Suspicion checks, guessing of hypotheses
- **Process data**: Attention checks, completion times, notes about unusual occurrences

**Tools & materials needed:**

- **Experimental materials**: Stimuli (videos, text, images, sounds), props, equipment specific to manipulation
- **Measurement instruments**: Questionnaires (online or paper), behavioral tasks, physiological recording equipment
- **Randomization tools**: Random number generators, experimental software with built-in randomization
- **Experimental software**: Qualtrics, PsychoPy, E-Prime, Inquisit, OpenSesame for stimulus presentation and data collection
- **Laboratory space**: Controlled environment (for lab experiments), computers, recording equipment
- **Online platforms**: MTurk, Prolific, CloudResearch for online experiments
- **Statistical software**: SPSS, R, Stata, Python for analysis
- **Recording equipment**: Video/audio recording (if analyzing behavior or verifying compliance)
- **Specialized equipment**: Eye-trackers, EEG, fMRI (for neuroscience experiments), VR equipment (for immersive environments)

---

## Sample Size Considerations

**Typical sample sizes:**

- **Small effects**: 200-800+ participants per condition
- **Medium effects**: 50-150 per condition
- **Large effects**: 20-50 per condition
- Within-subjects designs require fewer participants than between-subjects (because each person serves as their own control, reducing error variance)

**Power analysis determines required sample:**

- Specify: Expected effect size, desired power (typically .80), significance level (typically .05), number of conditions, design type
- Effect sizes from literature: If prior research found Cohen's d = 0.5, power analysis shows how many participants needed to reliably detect similar effects
- Pilot studies: Can provide preliminary effect size estimates, though small pilots produce uncertain estimates
- Conservative approach: Aim for larger samples than minimum required—insurance against smaller-than-expected effects

**Factors increasing required sample size:**

- Multiple conditions (each needs adequate n)
- Multiple dependent variables (particularly with correction for multiple testing)
- Subgroup analyses or moderators (need adequate n within each subgroup)
- Exploratory analyses beyond pre-registered hypotheses
- Noisy or unreliable measures (increase error variance requiring larger samples to detect signal)
- Small expected effects (subtle manipulations or distal outcomes)

**The replication crisis consideration**: Psychology and related fields have discovered many classic findings don't replicate—often because original studies were underpowered, effect sizes were overestimated, or publication bias favored significant results. Contemporary standards increasingly demand larger samples, pre-registration, and direct replications before treating effects as established.

---

## Analysis Approaches

**Common analytical techniques:**

**Basic designs:**

- **Independent samples t-test**: Comparing two independent groups (between-subjects)—tests whether means differ significantly
- **Paired samples t-test**: Comparing same participants across two conditions (within-subjects)—controls for individual differences
- **One-way ANOVA**: Comparing three or more independent groups—omnibus test followed by post-hoc comparisons
- **Repeated measures ANOVA**: Comparing same participants across multiple time points or conditions

**Complex designs:**

- **Factorial ANOVA**: Examining main effects of multiple independent variables and their interactions—reveals whether one variable's effect depends on another
- **ANCOVA (Analysis of Covariance)**: Including covariates (pre-existing individual differences) to reduce error variance and test effects controlling for confounds
- **MANOVA (Multivariate Analysis of Variance)**: Multiple dependent variables analyzed simultaneously—controls for familywise error when testing multiple outcomes
- **Mixed-effects models**: For designs combining between and within-subjects factors, or for nested data (participants within groups)

**Mediation and moderation:**

- **Mediation analysis**: Testing whether effects operate through intermediary variables (mechanisms)—does X → M → Y?
- **Moderation analysis**: Testing whether effects differ by subgroups or levels of another variable—does the effect of X on Y depend on Z?
- **Conditional process analysis**: Combining mediation and moderation in complex models

**Advanced approaches:**

- **Structural equation modeling**: Testing complex theoretical models with multiple relationships simultaneously
- **Latent variable models**: Accounting for measurement error in constructs
- **Regression discontinuity**: For natural experiments exploiting cutoff rules
- **Difference-in-differences**: For quasi-experiments comparing changes over time

**Effect size estimation:**

- **Cohen's d**: Standardized mean difference (small: 0.2, medium: 0.5, large: 0.8)
- **Partial eta-squared (η²)**: Proportion of variance explained in ANOVA
- **Odds ratios**: For binary outcomes
- **Confidence intervals**: Around effect estimates showing precision and uncertainty

**Software tools:**

- **SPSS**: User-friendly, widely used, good for standard analyses
- **R**: Free, flexible, powerful—packages like lme4 (mixed models), lavaan (SEM), mediation—steep learning curve
- **Stata**: Excellent for econometric approaches, regression discontinuity
- **Python**: Increasingly popular, statsmodels and scipy for statistics
- **G*Power**: Free software for power analysis
- **PROCESS macro** (Hayes): Popular tool for mediation and moderation in SPSS, SAS, R

---

## Strengths

- ✓ **Strongest causal inference**: Random assignment plus manipulation enables confident conclusions that changes in independent variables cause changes in dependent variables—ruling out reverse causality and many confounds
    
- ✓ **Control over extraneous variables**: Through randomization, standardization, and experimental control, experiments isolate effects of specific variables from confounding factors—attributing effects unambiguously to manipulations
    
- ✓ **Replicability**: Standardized procedures enable direct replication by independent researchers—essential for cumulative science and distinguishing robust findings from false positives
    
- ✓ **Precision about effect sizes**: Not just whether effects exist but how large—enabling meta-analysis, comparison across studies, and assessment of practical importance
    
- ✓ **Theory testing**: Experiments can definitively test specific theoretical predictions—falsifying or supporting theories through targeted hypothesis tests
    
- ✓ **Efficiency**: Well-designed experiments can answer questions relatively quickly compared to longitudinal observation or waiting for natural variation
    
- ✓ **Ethical intervention testing**: For evaluating treatments, policies, or programs, randomized experiments (RCTs) provide the most rigorous evidence about effectiveness—preventing adoption of ineffective or harmful interventions
    
- ✓ **Mechanistic insights**: Factorial designs and mediation analyses can reveal not just whether interventions work but how and why—illuminating underlying processes
    

---

## Limitations

- ✗ **Limited ecological validity**: Laboratory settings, artificial manipulations, and controlled conditions may produce results that don't generalize to messy real-world contexts—what works in the lab may not work in life
    
- ✗ **Demand characteristics**: Participants may guess hypotheses and alter behavior to comply or rebel—reactivity undermines validity. Being observed in experiments changes behavior in ways that may not occur naturally.
    
- ✗ **Ethical and practical constraints on manipulation**: Many important variables cannot or should not be experimentally manipulated—trauma, discrimination, poverty, long-term outcomes, rare events. This limits questions experiments can address.
    
- ✗ **Construct validity challenges**: Operationalizations may not capture theoretical constructs—"stress" induced in the lab differs from chronic life stress; "discrimination" in vignettes differs from lived experience. Findings may not generalize to different operationalizations.
    
- ✗ **Reductionism**: Isolating variables through experimental control may miss the very complexity and context that matters in real social life—interactions, feedback loops, emergence, culture. Understanding may require holism experiments sacrifice.
    
- ✗ **Sample limitations**: Convenience samples (students, MTurk) limit generalizability to broader populations. WEIRD samples may not reflect universal human psychology.
    
- ✗ **Short time frames**: Most experiments are brief (minutes to hours), measuring immediate outcomes. Long-term effects, development, or sustained behavior change are harder to study experimentally.
    
- ✗ **Statistical inference challenges**: P-values are widely misunderstood; statistical significance doesn't equal practical importance; multiple testing inflates false positive rates; publication bias favors significant results, distorting literature
    
- ✗ **Small effects and noise**: Many social/psychological effects are small and noisy—requiring large samples but often studied with insufficient power, leading to unreliable literatures
    
- ✗ **Limited understanding of mechanisms**: Experiments show whether X affects Y but often don't reveal how or why—black box approach unless mediators are explicitly measured and tested
    

---

## Ethical Considerations

- **Informed consent tensions**: Full disclosure about hypotheses may undermine experiments (participants alter behavior if knowing true purpose), yet deception compromises autonomy. Equipoise helps justify randomization, but when is withholding effective treatments from controls unethical?
    
- **Deception**: Common in psychological experiments (misleading about purpose, confederates posing as participants, false feedback), increasingly controversial. Can deception be justified if debriefing restores understanding? Or does it fundamentally violate respect for persons? Some argue deception has become overused and lazy—better designs could avoid it.
    
- **Random assignment ethics**: Is it fair that some receive beneficial treatments while controls don't? Justifiable when genuine uncertainty exists about which is better (equipoise), but not when withholding known effective treatments. Waitlist controls (eventually receiving treatment) and preference trials (allowing choice) address this.
    
- **Psychological harm**: Stress inductions, failure feedback, exclusion manipulations, disturbing stimuli can cause distress. Researchers must minimize harm, provide adequate debriefing, ensure participants leave in good condition. But can brief debriefing fully undo harm?
    
- **Vulnerable populations**: Experiments with children, prisoners, people with cognitive impairments, or economically desperate individuals raise special concerns about voluntariness and exploitation. Strict protections needed but not always sufficient.
    
- **Power and exploitation**: Researchers benefit (publications, careers) from participants' time and data. What reciprocity is owed? Monetary compensation is standard but may constitute coercion for desperate populations. What constitutes fair compensation versus undue inducement?
    
- **Privacy and confidentiality**: Behavioral experiments may reveal sensitive information. Online experiments raise additional concerns—data security, platform access to responses, linking experimental with other personal data.
    
- **Field experiment ethics**: Interventions in natural settings (workplaces, schools, communities) may affect people who didn't consent. Cluster randomization (randomizing groups rather than individuals) complicates consent. When can researchers intervene without individual consent?
    
- **Publication ethics**: Selective reporting, p-hacking (analyzing data multiple ways until finding significance), HARKing (Hypothesizing After Results Known) distort literature. Pre-registration helps but isn't foolproof. Obligation to report all conditions, all measures, all studies conducted?
    

---

## Validity & Reliability

Experiments face multiple validity threats that careful design must address:

**Internal validity (causal inference):**

- **Random assignment**: Primary control for confounds—ensures groups probabilistically equivalent
- **Manipulation checks**: Verify manipulation worked as intended—if not, results uninterpretable
- **Attention checks**: Ensure participants engaged with materials (especially online experiments where inattention is common)
- **Experimenter blindness**: Prevent unconscious bias from experimenters knowing conditions
- **Standardization**: Identical procedures except for manipulation—reduces error and alternative explanations

**Threats to internal validity:**

- **Selection bias**: If randomization fails or participants drop out differentially by condition
- **History**: External events occurring during experiment affecting outcomes
- **Maturation**: Changes in participants over time (fatigue, boredom, practice)—particularly concern for long experiments
- **Testing effects**: Pre-tests influencing post-test responses
- **Instrumentation**: Changes in measurement over time or across conditions
- **Regression to the mean**: Extreme scores naturally moving toward average—mistaken for treatment effects
- **Demand characteristics**: Participants altering behavior based on perceived experimental expectations

**External validity (generalizability):**

- **Population**: Do findings generalize beyond the studied sample to broader populations?
- **Ecological**: Do findings generalize from artificial experimental settings to real-world contexts?
- **Temporal**: Do effects persist over time beyond immediate post-test?
- **Construct**: Do findings generalize to other operationalizations of theoretical constructs?

**Construct validity:**

- **Manipulation validity**: Does manipulation capture intended construct? Alternative operationalizations produce same effects?
- **Measurement validity**: Do outcomes measure intended constructs? Reliable and valid instruments?
- **Confound-free**: Is manipulation unconfounded—varying only the intended factor without introducing other differences?

**Statistical conclusion validity:**

- **Adequate power**: Sufficient sample size to detect effects
- **Appropriate statistics**: Using correct tests for design and data
- **Assumption testing**: Verifying data meet statistical assumptions
- **Multiple testing correction**: When conducting many tests, controlling false positive rate (Bonferroni, FDR correction)

**Reliability:**

- **Test-retest reliability**: Same participants showing consistent responses over time (for within-subjects measures)
- **Inter-rater reliability**: Multiple coders/observers showing high agreement (for observational measures)
- **Internal consistency**: Multi-item scales showing high Cronbach's alpha

**Contemporary validity concerns:**

- **Replication failures**: Many published findings don't replicate—suggests original studies had false positives, publication bias, or context-specific effects treated as universal
- **Questionable research practices**: P-hacking, selective reporting, optional stopping distort literature
- **Pre-registration and registered reports**: Committing to design and analysis before seeing data reduces bias—increasingly required or encouraged

---

## Real-World Example

**Study title:** "Audit Study on Racial Discrimination in Hiring"

**Researchers:** Bertrand and Mullainathan (2004)

**What they did:** Researchers conducted a field experiment testing racial discrimination in hiring. They created fictitious résumés of equal objective quality (education, experience, skills) and randomly assigned stereotypically African-American names (Lakisha, Jamal) or stereotypically White names (Emily, Greg) to each résumé. In pairs, they sent one "Black" résumé and one "White" résumé to over 1,300 help-wanted advertisements in Chicago and Boston newspapers across various occupations and industries.

The manipulation was subtle but powerful: everything about résumés was identical except names signaling race—isolating race as the causal factor. Dependent variable was callback rate—whether employers contacted applicants for interviews. This was measured objectively through voicemail and email responses, eliminating subjective judgment in outcome measurement.

They randomly varied résumé quality as well (high vs. low), creating a 2×2 design testing whether discrimination varied by résumé quality. Multiple résumés per name reduced idiosyncratic name effects.

**Key findings:** White names received 50% more callbacks than African-American names (9.65% vs. 6.45%)—a substantial, statistically significant difference. This effect held across occupations, industries, and cities. Strikingly, résumé quality mattered more for White applicants than Black applicants—improving credentials increased callbacks substantially for White names but only marginally for Black names, suggesting discrimination may be even worse for high-quality Black applicants.

The field experiment design enabled strong causal inference: racial discrimination in hiring exists and is substantial. Employers' real hiring decisions revealed preferences that self-reports or correlational studies might miss or misattribute. The findings influenced policy debates and highlighted ongoing discrimination despite equal opportunity laws.

**Citation:** Bertrand, M., & Mullainathan, S. (2004). "Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination." _American Economic Review_, 94(4), 991-1013.

**Why this exemplifies experiments:** Random assignment of names isolated race as causal factor; manipulation was realistic (actual hiring decisions, not hypothetical scenarios); objective outcome measure (callbacks) avoided subjective bias; field setting provided ecological validity while maintaining experimental control; large sample enabled detection of policy-relevant effects; design ruled out alternative explanations (résumé quality, industries, cities); findings had clear theoretical and practical implications; study has been highly influential and replicated internationally.

---

## Getting Started: Practical Tips

1. **Start simple**: Your first experiment should test one clear hypothesis with straightforward manipulation and measurement. Resist the temptation to include multiple manipulations, mediators, moderators, and DVs—complexity makes interpretation difficult and increases failure risk.
    
2. **Pilot extensively**: Run preliminary versions with small samples (20-30) before full data collection. Check whether manipulations work, instructions are clear, measures make sense, timing is appropriate. Revise based on pilot feedback. This investment prevents wasting resources on flawed designs.
    
3. **Conduct a priori power analysis**: Don't guess sample size—calculate requirements based on expected effect size, desired power, and design. Underpowered studies waste resources and produce unreliable results. Use G*Power or online calculators; err toward larger samples.
    
4. **Pre-register when possible**: Before collecting data, publicly document hypotheses, design, sample size justification, and analysis plan (on OSF, AsPredicted). This prevents questionable practices and increases credibility. For student projects or exploratory research, at least document plans privately before data collection.
    
5. **Use established, validated measures**: Don't create new scales unless absolutely necessary—use measures with demonstrated reliability and validity. This enables comparison to prior research and ensures measurement quality.
    
6. **Include manipulation checks**: Build in verification that participants perceived manipulation as intended. If manipulation didn't work, results are uninterpretable. Better to discover this through manipulation checks than to publish findings misattributing effects.
    
7. **Attention checks for online experiments**: Include questions verifying participants read instructions ("Select 'strongly agree' for this item"). Screen out inattentive responders before analysis—they add noise and may not have received the manipulation.
    
8. **Randomize and counterbalance**: Use true randomization (software random assignment, not haphazard allocation). For within-subjects designs, counterbalance or randomize condition order controlling for order effects.
    
9. **Standardize everything except manipulation**: Same instructions, same setting, same experimenter behavior (or blind experimenters), same materials—except for the manipulated factor. Minimizes error variance and alternative explanations.
    
10. **Report all conditions, measures, and exclusions**: Transparency is essential. Document: all conditions run, all measures collected, all participants excluded and why, all analyses conducted. Selective reporting distorts literature—be honest about what you did.
    
11. **Focus on effect sizes, not just p-values**: Statistical significance doesn't equal importance. Always report and interpret effect sizes (Cohen's d, partial eta-squared). Small effects may be significant with large samples but practically meaningless.
    
12. **Consider limitations proactively**: Every experiment has limitations—operationalization choices, sample restrictions, artificial settings, short time frames. Acknowledge these honestly and discuss implications for interpretation. Self-awareness strengthens rather than weakens research.
    

---

## Common Mistakes to Avoid

- ⚠️ **Underpowered studies**: Collecting too few participants to reliably detect effects—produces unreliable results, wastes resources, contributes to replication crisis. Always conduct power analysis and collect adequate samples.
    
- ⚠️ **Confounded manipulations**: Varying multiple factors simultaneously when intending to manipulate one—makes effects uninterpretable. Ensure manipulation changes only intended construct. Example: using different speakers in audio conditions (confounding speaker with content).
    
- ⚠️ **Weak or ambiguous manipulations**: Subtle manipulations participants don't notice or perceive differently than intended. Manipulation checks are essential—without them, you don't know if treatment was received.
    
- ⚠️ **Demand characteristics**: Overly obvious manipulations telegraphing hypotheses, leading participants to respond according to perceived expectations rather than genuinely. Use cover stories, embed target items among fillers, keep participants naive to hypotheses.
    
- ⚠️ **P-hacking**: Analyzing data multiple ways, testing multiple DVs without correction, stopping data collection when reaching significance, excluding outliers selectively—inflates false positive rate. Pre-register to prevent this.
    
- ⚠️ **HARKing (Hypothesizing After Results Known)**: Presenting exploratory findings as if they were predicted a priori. Clearly distinguish confirmatory from exploratory analyses—both are valuable but must be labeled honestly.
    
- ⚠️ **Ignoring failed manipulation checks**: Proceeding with analysis when manipulation checks show treatment didn't work as intended. If manipulation failed, results are meaningless—either fix manipulation and recollect or acknowledge failure.
    
- ⚠️ **Over-generalizing**: Claiming findings from student samples generalize to all humans, or from one operationalization to all instances of a construct. Be appropriately tentative about scope of conclusions.
    
- ⚠️ **Confusing statistical with practical significance**: Treating tiny effects as meaningful because they're statistically significant with large samples. Always interpret effect sizes practically—does this magnitude matter in the real world?
    
- ⚠️ **Not reporting non-significant results**: Failing to publish or report null findings contributes to publication bias. Null results are informative—they help map boundary conditions and prevent overconfidence in effects.
    
- ⚠️ **Inadequate debriefing**: Especially with deception or distressing manipulations, superficial debriefing leaves participants confused or upset. Ensure participants understand purpose, feel comfortable, and leave in good condition.
    
- ⚠️ **File drawer problem**: Running multiple studies, publishing only significant results. This distorts literature, creating false impressions of robust effects when reality is mixed. Report all studies conducted.
    

---

## Resource Requirements

**Time:**

- Simple lab experiment: 3-6 months (design, IRB, data collection, analysis)
- Online experiment: 2-4 months (faster recruitment and data collection)
- Complex multi-session experiment: 6-12 months
- Field experiment: 6-18 months (coordination, implementation in real settings) Add time for: Pilot testing (1-2 months), IRB approval (1-3 months), analysis and writing (2-4 months)

**Budget:** Highly variable

- Minimal online experiment: $500-$2,000 (participant compensation, survey platform)
- Standard lab experiment: $3,000-$10,000 (participant payments, equipment, software, lab space)
- Large-scale online experiment: $5,000-$20,000 (many participants, multiple studies)
- Field experiment: $20,000-$100,000+ (intervention delivery, staff, coordination, long-term follow-up)
- Neuroscience experiment: $50,000-$200,000+ (fMRI scanning costs, specialized equipment)

Major costs: Participant compensation ($5-$50 per participant depending on length and effort), online platform fees (MTurk takes 40% fee, Prolific 33%), software licenses, equipment, lab space, research assistant time, specialized equipment rental (eye-tracking, physiological recording)

**Skills needed:**

- Experimental design logic and principles
- Statistical analysis capabilities
- Programming/software skills for stimulus presentation
- Attention to procedural detail and standardization
- Ethical reasoning about manipulation and deception
- Critical thinking about construct validity and generalization
- Clear communication of complex designs
- Patience for iterative development and piloting

**Team size:**

- Minimum: 1 (possible for online experiments, though challenging to maintain blind conditions)
- Typical: 2-4 (PI, research assistants for data collection, possibly co-investigators)
- Large: 5-15+ (field experiments with multiple sites, implementation staff, coordinated data collection)

---

## Combining with Other Methods

**Works well with:**

- **Surveys**: Experiments identify causal effects; surveys assess prevalence, correlates, and generalizability to broader populations. Sequential designs: survey identifies patterns, experiment tests causation; or experiment establishes effect, survey measures how common it is naturally.
    
- **Interviews or focus groups**: Qualitative methods explore mechanisms, interpretations, and experiences. Interviews after experiments: understand how participants interpreted manipulations, what thought processes occurred, why effects emerged. Focus groups before experiments: generate realistic stimuli, identify appropriate operationalizations.
    
- **Observational studies**: Complement experimental causation with naturalistic description. Ethnography shows how phenomena operate in real contexts; experiments test specific causal claims about those phenomena in controlled settings.
    
- **Content analysis**: Analyze naturally occurring content, then experimentally manipulate those elements to test effects. Example: content analyze media frames, then experiment exposing participants to different frames testing effects on attitudes.
    
- **Meta-analysis**: Systematic synthesis across multiple experiments revealing cumulative evidence, moderators, boundary conditions. Single experiments are limited; meta-analysis of many reveals robustness and generalizability.
    
- **Neuroimaging**: Combining experimental manipulation with fMRI, EEG, or other neuroscience methods reveals neural mechanisms underlying behavioral effects—adding biological validity to psychological theories.
    
- **Longitudinal designs**: Most experiments are cross-sectional; combining with longitudinal follow-up assesses whether effects persist, emerge over time, or dissipate.
    

---

## Further Reading

**Essential texts:**

1. Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). _Experimental and Quasi-Experimental Designs for Generalized Causal Inference_. Houghton Mifflin. [Definitive, comprehensive treatment of experimental design, validity threats, causal inference; essential reference]
    
2. Field, A., & Hole, G. (2003). _How to Design and Report Experiments_. SAGE Publications. [Accessible introduction covering design, conduct, analysis, and reporting; practical guidance]
    
3. Gerring, J., & Christenson, D. (2017). _Applied Social Science Methodology: An Introductory Guide_. Cambridge University Press. [Includes strong chapters on experiments in social science context]
    
4. Morton, R. B., & Williams, K. C. (2010). _Experimental Political Science and the Study of Causality_. Cambridge University Press. [Political science perspective on experiments; strong on field experiments and causation]
    
5. Glennerster, R., & Takavarasha, K. (2013). _Running Randomized Evaluations: A Practical Guide_. Princeton University Press. [Field experiments and randomized controlled trials; practical implementation focus]
    
6. Dunning, T. (2012). _Natural Experiments in the Social Sciences_. Cambridge University Press. [Natural and quasi-experiments exploiting naturally occurring variation]
    
7. Gerber, A. S., & Green, D. P. (2012). _Field Experiments: Design, Analysis, and Interpretation_. W.W. Norton. [Essential guide to field experiments; covers design, implementation, analysis]
    

**Methodological articles:**

8. Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant." _Psychological Science_, 22(11), 1359-1366. [Influential article demonstrating how questionable practices inflate false positives; essential reading]
    
9. Nosek, B. A., et al. (2015). "Promoting an Open Research Culture." _Science_, 348(6242), 1422-1425. [Discusses pre-registration, replication, transparency improving research quality]
    

**Online resources:**

- OSF (Open Science Framework): Platform for pre-registration, data sharing, materials (osf.io)
- AsPredicted: Simple pre-registration platform (aspredicted.org)
- G*Power: Free power analysis software
- Center for Open Science: Resources on open science practices

**Key journals:**

- _Journal of Experimental Psychology_ (multiple sections)
- _Experimental Economics_
- _Journal of Experimental Social Psychology_
- _Political Behavior_ (many experiments)
- _Quarterly Journal of Experimental Psychology_

---

## Related Methods

- **Quasi-experiments** - Studies with experimental manipulation but lacking random assignment. Examples: regression discontinuity (exploiting cutoff rules), difference-in-differences (comparing changes over time), interrupted time series. Weaker causal inference than true experiments but useful when randomization impossible.
    
- **Natural experiments** - Exploiting naturally occurring events creating quasi-random variation—policy changes, natural disasters, arbitrary rules. No researcher manipulation but variation approximates randomization. Examples: lottery systems, birthday cutoffs, geographic boundaries.
    
- **Randomized controlled trials (RCTs)** - Medical and policy evaluation terminology for experiments testing interventions. Same logic as experiments but often larger scale, longer duration, in field settings, with intention-to-treat analysis.
    
- **A/B testing** - Industry terminology (tech, marketing) for experiments comparing two versions. Online platforms routinely experiment on users testing features, designs, algorithms. Raises ethical questions about consent and manipulation.
    
- **Laboratory experiments versus field experiments** - Lab experiments: Controlled laboratory settings maximizing internal validity but potentially sacrificing ecological validity. Field experiments: Natural settings (schools, workplaces, communities) sacrificing some control for realism. Trade-offs between causal certainty and generalizability.
    
- **Factorial experiments** - Manipulating multiple independent variables simultaneously (2×2, 2×3, 2×2×2 designs) testing main effects and interactions. More complex but reveals whether effects depend on other factors.
    
- **Within-subjects (repeated measures) versus between-subjects designs** - Within: Same participants in all conditions (efficient, controls individual differences, vulnerable to order effects). Between: Different participants per condition (simpler analysis, avoids carryover, requires larger samples).
    
- **Mixed methods experiments** - Combining experimental manipulation with qualitative methods (interviews, observation, open-ended responses) to understand mechanisms, interpretations, and experiences alongside causal effects.
    

---

**Last updated:** October 26, 2025